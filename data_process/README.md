# Introduction

Some data processing scripts for the reporting system (perf-insight).  
They pick data from pbench-agent output, do further analysis, deal with additional metadata and finally convert them into reporting system wanted format. The scripts are designed to keep the current pbench data model.

# Usage

## Before you start

In this project, most of the scripts take the `datastore.json` as an input. This json file is composed by collecting "iteration data" generated by the pbench-agent and injected with some additional information. So it keeps the same data model as the pbench project.

Run the following script to generate the `datastore.json`:

```bash
$ ll ./data_source/fio_ESXi_RHEL-8.3.0-2020111009.2_lite_scsi_D201203T114029
total 7.0M
drwxr-xr-x. 6 root root 4.0K Dec  9 16:38 fio_ESXi_RHEL-8.3.0-2020111009.2_lite_scsi_D201203T114029_2020.12.03T03.40.30
drwxr-xr-x. 6 root root 4.0K Dec  9 16:38 fio_ESXi_RHEL-8.3.0-2020111009.2_lite_scsi_D201203T114029_2020.12.03T03.52.20
drwxr-xr-x. 6 root root 4.0K Dec  9 16:38 fio_ESXi_RHEL-8.3.0-2020111009.2_lite_scsi_D201203T114029_2020.12.03T04.04.42
drwxr-xr-x. 6 root root 4.0K Dec  9 16:38 fio_ESXi_RHEL-8.3.0-2020111009.2_lite_scsi_D201203T114029_2020.12.03T04.15.34
-rw-r--r--. 1 root root  603 Dec  9 16:41 metadata.json

$ ./gather_testrun_datastore.py \
    --logdir ./data_source/fio_ESXi_RHEL-8.3.0-2020111009.2_lite_scsi_D201203T114029/ \
    --testrun fio_ESXi_RHEL-8.3.0-2020111009.2_lite_scsi_D201203T114029 \
    --output ./workspace/datastore.json

$ ll ./workspace 
total 6.9M
-rw-rw-r--. 1 cheshi cheshi 6.9M Dec 16 13:29 datastore.json
```

## Flask

### Preparation

Before you start, you need to prepare the following stuff:
1. datastore.json - contains the performance data of a testrun.
2. metadata.json - contains the metadata of a testrun.
3. generate_testrun_results.yaml - user config.

The following scripts are needed:
1. generate_testrun_results.py - generate a CSV file for the database to load.
2. load_db.py (TBD) - load the data into the Flask database.

### Steps

Run the following script to generate the CSV file:

```bash
$ ./generate_testrun_results.py \
    --config ./templates/generate_testrun_results-flask_fio.yaml \
    --datastore ./workspace/datastore.json \
    --metadata ./data_source/fio_ESXi_RHEL-8.3.0-2020111009.2_lite_scsi_D201203T114029/metadata.json \
    --output-format csv \
    --output ./workspace/testrun_results.csv
```

The `testrun_results.csv` will contain all the fields which the flask database needs.

Run the following script to load the data into database:

```bash
# To be done from Frank side.
```

## Jupyter

### Preparation

Before you start, you need to prepare the following stuff:
1. datastore.json - contains the performance data of a testrun.
2. metadata.json - contains the metadata of a testrun.
3. generate_testrun_results.yaml - user config for generating test summary.
4. generate_2way_benchmark.yaml - user config for generating benchmark reports.

**Notes:** in a typical 2-way comparison, the `datastore.json` and `metadata.json` are needed for both TEST and BASE runs.

The following scripts are needed:
1. generate_testrun_results.py - generate the test summary for Jupyter to display.
2. generate_2way_metadata.py - generate the metadata comparison for Jupyter to display.
3. generate_benchmark_config.py (TBD) - generate the benchmark config for Jupyter to display.
4. generate_2way_benchmark.py - generate the benchmark comparison for Jupyter to display.

### Steps

```bash
$ ll ./workspace/
total 32M
-rw-rw-r--. 1 cheshi cheshi 6.9M Dec 16 13:29 base.datastore.json
-rw-r--r--. 1 cheshi cheshi  626 Dec 16 13:55 base.metadata.json
-rw-r--r--. 1 cheshi cheshi  25M Dec 16 13:56 test.datastore.json
-rw-r--r--. 1 cheshi cheshi  620 Dec 16 13:57 test.metadata.json
lrwxrwxrwx. 1 cheshi cheshi   31 Dec 16 14:33 generate_2way_benchmark.yaml -> ../generate_2way_benchmark.yaml
lrwxrwxrwx. 1 cheshi cheshi   54 Dec 16 14:32 generate_testrun_results.yaml -> ../templates/generate_testrun_results-jupyter_fio.yaml
```

Run the following script to generate the test summary:

```bash
$ ./generate_testrun_results.py \
    --config ./workspace/generate_testrun_results.yaml \
    --datastore ./workspace/test.datastore.json \
    --metadata ./workspace/test.metadata.json \
    --output-format csv \
    --output ./workspace/test.testrun_results.csv
```

Do it again for the BASE run.

Run the following script to generate the metadata comparison:

```bash
$ ./generate_2way_metadata.py \
    --test ./workspace/test.metadata.json \
    --base ./workspace/base.metadata.json \
    --output-format csv \
    --output ./workspace/2way_metadata.csv
```

Run the following script to generate the benchmark parameters:

```bash
$ ./generate_2way_parameters.py \
    --benchmark_config ./workspace/generate_2way_benchmark.yaml \
    --output-format csv \
    --output ./workspace/2way_parameters.csv
```

Run the following script to generate the benchmark comparison:

```bash
$ ./generate_2way_benchmark.py \
    --config ./workspace/generate_2way_benchmark.yaml \
    --test ./workspace/test.testrun_results.csv \
    --base ./workspace/base.testrun_results.csv \
    --output-format csv \
    --output ./workspace/2way_benchmark.csv
```
