{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FLASK_ADDR = \"http://10.73.196.185:5000\"\n",
    "\n",
    "WORKSPACE = \"/workspace\"\n",
    "YAML = WORKSPACE + \"/benchmark_config.yaml\"\n",
    "BASE_DATASTORE = WORKSPACE + \"/base.datastore.json\"\n",
    "TEST_DATASTORE = WORKSPACE + \"/test.datastore.json\"\n",
    "BASE_METADATA = WORKSPACE + \"/base.testrun_metadata.json\"\n",
    "TEST_METADATA = WORKSPACE + \"/test.testrun_metadata.json\"\n",
    "\n",
    "BASE_TESTRUN_RESULT = WORKSPACE + \"/base.testrun_result.csv\"\n",
    "TEST_TESTRUN_RESULT = WORKSPACE + \"/test.testrun_result.csv\"\n",
    "METADATA = WORKSPACE + \"/2way_metadata.csv\"\n",
    "BENCHMARK = WORKSPACE + \"/2way_benchmark.csv\"\n",
    "PARAMETERS = WORKSPACE + \"/2way_parameters.csv\"\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import yaml\n",
    "import json\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML, Markdown\n",
    "from datetime import datetime\n",
    "\n",
    "BASEPATH = os.path.abspath('.')\n",
    "SCRIPTPATH = BASEPATH + \"/../data_process\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance Test Report Portal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "now = datetime.now()\n",
    "dt_string = \"*Generate time: {}*\".format(now.strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "display(Markdown(dt_string))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_json(json_file):\n",
    "    with open(json_file, 'r') as f:\n",
    "        try:\n",
    "            data = json.load(f)\n",
    "        except Exception as e:\n",
    "            print(\"Fail to load {}\".format(json_file))\n",
    "            raise\n",
    "    return data\n",
    "\n",
    "base_metadata = read_json(BASE_METADATA)\n",
    "test_metadata = read_json(TEST_METADATA)\n",
    "assert base_metadata.get(\"testrun.type\") == test_metadata.get(\"testrun.type\"), \"Base and Test type must be the same! Exit.\"\n",
    "\n",
    "run_type = base_metadata.get(\"testrun.type\")\n",
    "base_platform = base_metadata.get(\"testrun.platform\")\n",
    "test_platform = test_metadata.get(\"testrun.platform\")\n",
    "base_id = base_metadata.get(\"testrun.id\")\n",
    "test_id = test_metadata.get(\"testrun.id\")\n",
    "# Type and platform must not be None\n",
    "assert run_type is not None, \"Type is None! Exit.\"\n",
    "assert base_platform is not None, \"Base platform is None! Exit.\"\n",
    "assert test_platform is not None, \"Test platform is None! Exit.\"\n",
    "\n",
    "with open('{}/templates/{}_{}.md'.format(BASEPATH, base_platform.lower(), run_type), 'r') as f:\n",
    "    display(Markdown(f.read()))\n",
    "    \n",
    "if base_platform != test_platform:\n",
    "    with open('{}/templates/{}_{}.md'.format(BASEPATH, test_platform.lower(), run_type), 'r') as f:\n",
    "        display(Markdown('\\n'+f.read()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%html\n",
    "<style>\n",
    "    table {\n",
    "        display: inline-block\n",
    "    }\n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Base testrun result script\n",
    "base_gen_result_script = \"{}/generate_testrun_results.py --config {} --datastore {} --metadata {} --output {}\".format(\n",
    "SCRIPTPATH, YAML, BASE_DATASTORE, BASE_METADATA, BASE_TESTRUN_RESULT)\n",
    "\n",
    "# Generate Test testrun result script\n",
    "test_gen_result_script = \"{}/generate_testrun_results.py --config {} --datastore {} --metadata {} --output {}\".format(\n",
    "SCRIPTPATH, YAML, TEST_DATASTORE, TEST_METADATA, TEST_TESTRUN_RESULT)\n",
    "\n",
    "# Generate 2way metadata script\n",
    "gen_metadata_script = \"{}/generate_2way_metadata.py --test {} --base {} --output {}\".format(\n",
    "SCRIPTPATH, TEST_METADATA, BASE_METADATA, METADATA)\n",
    "\n",
    "# Generate 2way benchmark script\n",
    "gen_benchmark_script = \"{}/generate_2way_benchmark.py --config {} --test {} --base {} --output {}\".format(\n",
    "SCRIPTPATH, YAML, TEST_TESTRUN_RESULT, BASE_TESTRUN_RESULT, BENCHMARK)\n",
    "\n",
    "# Generate 2way parameters script\n",
    "gen_parameters_script = \"{}/generate_2way_parameters.py --benchmark_config {} --output {}\".format(\n",
    "SCRIPTPATH, YAML, PARAMETERS)\n",
    "\n",
    "# Run scripts parallelly\n",
    "import multiprocessing\n",
    "all_processes = (base_gen_result_script, test_gen_result_script, gen_metadata_script, gen_parameters_script)   \n",
    "\n",
    "def execute(process):                                                             \n",
    "    os.system(f'python3 {process}') \n",
    "\n",
    "process_pool = multiprocessing.Pool(processes = 4)                                                        \n",
    "process_pool.map(execute, all_processes)\n",
    "\n",
    "for result in [BASE_TESTRUN_RESULT, TEST_TESTRUN_RESULT, METADATA]:\n",
    "    assert os.path.exists(result), \"Fail to generate {}! Exit.\".format(result)\n",
    "\n",
    "# Generate 2way benchmark\n",
    "os.system('python3 {}'.format(gen_benchmark_script))\n",
    "assert os.path.exists(BENCHMARK), \"Fail to generate {}! Exit.\".format(BENCHMARK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def color_delta(val):\n",
    "    with open(YAML) as f:\n",
    "        fillna = yaml.safe_load(f).get('benchmark_comparison_generator').get('defaults').get('fillna', \"NaN\")\n",
    "    color_dict = {\n",
    "        \"DR\": 'color: red; background-color: #FFB6C1',\n",
    "        \"MR\": 'color: black',\n",
    "        \"DI\": 'color: green; background-color: #F0FFF0',\n",
    "        \"MI\": 'color: black',\n",
    "        \"HV\": 'color: orange; background-color: #FAFAD2',\n",
    "        \"NS\": 'color: gray',\n",
    "        \"NC\": 'color: gray',\n",
    "        \"ID\": 'color: red',\n",
    "        fillna: 'color: #D3D3D3',\n",
    "        \n",
    "    }\n",
    "    return '{}'.format(color_dict.get(val, 'color: black'))\n",
    "\n",
    "def highlight_cols(s):\n",
    "    return 'background-color: #eeffff'\n",
    "\n",
    "def bold_font(s):\n",
    "    return 'font-weight: bold'\n",
    "\n",
    "def displayComparison(df):\n",
    "    #These are the columns which need special formatting\n",
    "    deltacols=df.columns.map(lambda x: x.endswith(\"-CON\"))\n",
    "#    roundcols=df.columns.map(lambda x: x.endswith((\"-AVG\", \"\", \"-%SD\", \"-%DIFF\", \"-SIGN\")))\n",
    "    display(df.style\\\n",
    "            .applymap(color_delta,subset=deltacols)\\\n",
    "            .applymap(bold_font,subset=deltacols)\\\n",
    "            .format({'Test': lambda x: '<a target=\"_blank\" href=\"{}\">link</a>'.format(x)})\\\n",
    "            .format({'Base': lambda x: '<a target=\"_blank\" href=\"{}\">link</a>'.format(x)}))\n",
    "#    display(df.style.applymap(color_delta,subset=deltacols).applymap(bold_font,subset=deltacols).format(FORMATER, subset=roundcols).hide_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%HTML\n",
    "* The differences between Test and Base are <b style='color:orange'>highlighted</b>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def highlight_diff(row, cell_format):\n",
    "    cell_format = cell_format if row['TEST'] != row['BASE'] else ''\n",
    "    format_row = ['', cell_format, cell_format]\n",
    "    return format_row\n",
    "\n",
    "def color_diff(row):\n",
    "    return highlight_diff(row, 'color: orange')\n",
    "\n",
    "def bold_diff(row):\n",
    "    return highlight_diff(row, 'font-weight: bold')\n",
    "    \n",
    "conf_df = pd.read_csv(METADATA, index_col=0, keep_default_na=False)\n",
    "conf_df = conf_df[['NAME', 'TEST', 'BASE']]\n",
    "#sorter = ['testrun.id'] + [x for x in conf_df['KEY'] if x != 'testrun.id']\n",
    "#conf_df['KEY'] = conf_df['KEY'].astype(\"category\")\n",
    "#conf_df[\"KEY\"].cat.set_categories(sorter, inplace=True)\n",
    "#conf_df.sort_values(['KEY'], inplace=True)\n",
    "#display(conf_df.style.applymap(bold_font, subset=['KEY']))\n",
    "display(conf_df.style.applymap(bold_font, subset=['NAME']).apply(color_diff, axis=1).apply(bold_diff, axis=1).hide_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "with open('{}/templates/benchmark_description.md'.format(BASEPATH), 'r') as f:\n",
    "    display(Markdown(f.read()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**User parameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(PARAMETERS) as f:\n",
    "    param_df = pd.read_csv(f, index_col=0, dtype=str, keep_default_na=False)\n",
    "    \n",
    "display(param_df.style.applymap(bold_font, subset=['name']).hide_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Benchmark Report**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def get_report_link(row, metadata_file):\n",
    "    with open(metadata_file) as f:\n",
    "        m = json.load(f)\n",
    "    info = {\n",
    "        \"flask_addr\": FLASK_ADDR,\n",
    "        \"testrun_id\": m.get('testrun_id'),\n",
    "        \"platform\": m.get('testrun.platform'),\n",
    "        \"backend\": m.get('disk.backend'),\n",
    "        'driver': m.get('disk.driver'),\n",
    "        'format': m.get('disk.format'),\n",
    "        'iodepth': row['IOdepth'],\n",
    "        'numjobs': row['Numjobs'],\n",
    "        'bs': row['BS'],\n",
    "        'rw': row['RW'],\n",
    "            }\n",
    "    return \"{flask_addr}/storageresultpubview/list/?\\\n",
    "_flt_3_testrun={testrun_id}&\\\n",
    "_flt_3_platform={platform}&\\\n",
    "_flt_3_backend={backend}&\\\n",
    "_flt_3_driver={driver}&\\\n",
    "_flt_3_format={format}&\\\n",
    "_flt_3_bs={bs}\\\n",
    "_flt_3_rw={rw}\\\n",
    "_flt_0_iodepth={iodepth}&\\\n",
    "_flt_0_numjobs={numjobs}&\\\n",
    "\".format(**info)\n",
    "\n",
    "benchmark_df = pd.read_csv(BENCHMARK, index_col=0, dtype=str, keep_default_na=False)\n",
    "#summary_df = benchmark_df[['RW','BS','IOdepth','Numjobs']+list(benchmark_df.filter(regex='-CON$').columns)]\n",
    "benchmark_df.insert(0, 'Test', benchmark_df.apply(lambda row: get_report_link(row, TEST_METADATA), axis=1))\n",
    "benchmark_df.insert(0, 'Base', benchmark_df.apply(lambda row: get_report_link(row, BASE_METADATA), axis=1))\n",
    "\n",
    "displayComparison(benchmark_df)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
